{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/oasis-lmf-colour.png\" alt=\"Oasis LMF logo\" width=\"250\" align=\"left\"/>\n",
    "<br><br><br>\n",
    "\n",
    "# Exercise 2:   Running a model in the Oasis MDK.\n",
    "\n",
    "The Oasis kernel provides a robust loss simulation engine for catastrophe modelling. Insurance practitioners are used to dealing with losses arising from events. These losses are numbers, not distributions. Policy terms are applied to the losses individually and then aggregated and further conditions or reinsurances applied. Oasis takes the same perspective, which is to generate individual losses from the probability distributions. The way to achieve this is random sampling called “Monte-Carlo” sampling from the use of random numbers, as if from a roulette wheel, to solve equations that are otherwise intractable.\n",
    "\n",
    "Modelled and empirical intensities and damage responses can show significant uncertainty, Sometimes this uncertainty is multi-modal, meaning that there can be different peaks of behaviour rather than just a single central behaviour. Moreover, the definition of the source insured interest characteristics, such as location or occupancy or construction, can be imprecise. The associated values for event intensities and consequential damages can therefore be varied and their uncertainty can be represented in general as probability distributions rather than point values. The design of Oasis therefore makes no assumptions about the probability distributions and instead treats all probability distributions as probability masses in discrete bins. This includes closed interval point bins such as the values [0,0] for no damage and [1,1] for total damage.\n",
    "\n",
    "The simulation approach taken by the Oasis calculation kernel computes a single cumulative distribution function (CDF) for the damage by “convolving” the binned intensity distribution with the vulnerability matrices. Sampling can then be done against the CDF. \n",
    "\n",
    "<img src=\"images/simulation_methodology.png\" alt=\"Oasis simulation methodology\" width=\"600\"/>\n",
    "\n",
    "The Oasis kernel requires a standard set of files for capturing the hazard footprints and vulnerability data.\n",
    "\n",
    "<img src=\"images/oasis_model_files.png\" alt=\"Oasis model files\" width=\"600\"/>\n",
    "\n",
    "The files are:\n",
    "\n",
    "#### area peril dictionary\n",
    "    The meta-data that describes the model specific geo-spatial grid. This can be a set of points, a regular grid or a variable resolutiuon grid.\n",
    "#### intensity bin dictionary\n",
    "    The meta-data that descibes the hazard intensities corresponding to the bins.\n",
    "#### hazard\n",
    "    The hazard values for each impacted area peril cell for each event in the stochastic catalogue.\n",
    "#### damage bin dictionary\n",
    "    The meta-data tha descibes the damage percentages corresponding to the bins.\n",
    "#### vulnerability dictionary\n",
    "    The meta-data that descibes the vulnerability data, in particular mapping particular curves to particular exposure attributes.\n",
    "#### vulnerability\n",
    "    The vulnerability data. \n",
    "#### event\n",
    "    The list of events in the stochastic catalogue. Event files can be use to distinguish event types, such as historical.\n",
    "#### occurrences\n",
    "    The list of event occurrences in particular periods, used for loss curve calculations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import numpy as np\n",
    "import json\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import os\n",
    "from shapely.geometry import Point, Polygon\n",
    "from numpy import linspace\n",
    "from bokeh import events\n",
    "from bokeh.io import output_file, show, output_notebook\n",
    "from bokeh.models import ColumnDataSource, FixedTicker, PrintfTickFormatter, CustomJS, Div, TapTool\n",
    "from bokeh.plotting import figure\n",
    "import branca.colormap as cm\n",
    "import datetime\n",
    "import psutil\n",
    "import time\n",
    "# Output Bokeh charts to notebook, rather than opening a browser window\n",
    "output_notebook()\n",
    "\n",
    "map_centre = [18.64, -70.09]\n",
    "map_zoom = 8\n",
    "\n",
    "import jupyter_helper\n",
    "jupyter_helper.set_style()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View the model files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Area peril dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_peril_dictionary = pd.read_csv(\"./gem/keys_data/GMO/areaperil_dict_pga_only.csv\")\n",
    "area_peril_dictionary.columns = [x.lower() for x in area_peril_dictionary.columns]\n",
    "area_peril_dictionary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that some browsers will not display the map as there are too many points to plot\n",
    "\n",
    "m = folium.Map(tiles='cartodbpositron', location=map_centre, zoom_start=map_zoom)\n",
    "for i, row in area_peril_dictionary.iterrows():\n",
    "    folium.CircleMarker(\n",
    "        location=[row.lat1, row.lon1], radius=1).add_to(m)\n",
    "m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intensity bin dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intensity_bin_dictionary = pd.read_csv(\"./gem/model_data/GMO/intensity_bin_dict.csv\")\n",
    "area_peril_dictionary.columns = [x.lower() for x in area_peril_dictionary.columns]\n",
    "intensity_bin_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_id = 2401\n",
    "area_peril_id = 15939"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To reduce memory usage we read the footprint file in chunks and filter according to event_id\n",
    "iter_csv = pd.read_csv(\"./gem/model_data/GMO/footprint.csv\", iterator=True, chunksize=1000000) \n",
    "footprints = pd.concat([chunk[chunk['event_id'] == event_id] for chunk in iter_csv])\n",
    "footprints.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprint_with_hazard = footprints.merge(\n",
    "    area_peril_dictionary, how='inner', \n",
    "    left_on='areaperil_id', right_on='area_peril_id').merge(\n",
    "    intensity_bin_dictionary, how=\"inner\",\n",
    "    left_on=\"intensity_bin_index\", right_on=\"bin_index\")\n",
    "footprint_with_hazard = footprint_with_hazard[['areaperil_id', 'lat1', 'lon1', 'prob', 'intensity_bin_index','interpolation']] \n",
    "footprint_with_hazard = footprint_with_hazard.sort_values(['areaperil_id'])\n",
    "footprint_with_hazard = footprint_with_hazard.rename(index=str, columns={\"interpolation\": \"hazard\"})\n",
    "footprint_with_hazard.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "footprint_with_hazard_for_cell = footprint_with_hazard[footprint_with_hazard.areaperil_id == area_peril_id] \n",
    "footprint_with_hazard_for_cell = intensity_bin_dictionary.merge(\n",
    "    footprint_with_hazard_for_cell, how=\"inner\",\n",
    "    left_on=\"bin_index\", right_on=\"intensity_bin_index\", suffixes=('', '_dict'))\n",
    "\n",
    "footprint_with_hazard_for_cell.fillna(0)\n",
    "footprint_with_hazard_for_cell = footprint_with_hazard_for_cell.sort_values(\"intensity_bin_index\")\n",
    "footprint_with_hazard_for_cell = footprint_with_hazard_for_cell[['prob', 'intensity_bin_index','interpolation']]\n",
    "footprint_with_hazard_for_cell = footprint_with_hazard_for_cell.rename(index=str, columns={\"interpolation\": \"hazard\"})\n",
    "footprint_with_hazard_for_cell.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Note that some browsers will not display the map as there are too many points to plot\n",
    "\n",
    "footprint_with_hazard['weighted_hazard'] = footprint_with_hazard['hazard'] * footprint_with_hazard['prob']\n",
    "footprint_with_mean_hazard = pd.DataFrame({'mean_hazard' : footprint_with_hazard.groupby( ['areaperil_id', 'lat1', 'lon1'] )['weighted_hazard'].sum()}).reset_index()\n",
    "linear = cm.LinearColormap(\n",
    "    ['green', 'yellow', 'red'],\n",
    "    vmin=min(footprint_with_mean_hazard.mean_hazard), \n",
    "    vmax=max(footprint_with_mean_hazard.mean_hazard))\n",
    "m = folium.Map(location=map_centre, zoom_start=map_zoom, tiles='cartodbpositron')\n",
    "for i, row in footprint_with_mean_hazard.iterrows():\n",
    "    c = linear(row.mean_hazard)\n",
    "    folium.CircleMarker(\n",
    "        location=[row.lat1, row.lon1], fill_color=c, radius=5,\n",
    "        weight=0, fill=True, fill_opacity=1.0).add_to(m)\n",
    "linear.caption = 'Hazard'\n",
    "m.add_child(linear)\n",
    "m.fit_bounds(m.get_bounds())\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "intensity_range = (0, footprint_with_hazard_for_cell.hazard.max())\n",
    "p = figure(x_range=intensity_range, plot_height=300, y_range=(0, footprint_with_hazard_for_cell.prob.max()), toolbar_location=None)\n",
    "p.vbar(x=footprint_with_hazard_for_cell.hazard, top=footprint_with_hazard_for_cell.prob, width=0.9)\n",
    "p.xaxis.axis_label = 'Hazard'\n",
    "p.yaxis.axis_label = 'Probability'\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Damage bin dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "damage_bin_dictionary = pd.read_csv(\"./gem/model_data/GMO/damage_bin_dict.csv\")\n",
    "damage_bin_dictionary.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerability dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vulnerability_dict = pd.read_csv(\"./gem/keys_data/GMO/vulnerability_dict_pga_only.csv\")\n",
    "vulnerability_dict.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vulnerabilities = pd.read_csv(\"./gem/model_data/GMO/vulnerability.csv\")\n",
    "vulnerabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Occurrence file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occurrences = pd.read_csv(\"gem/model_data/GMO/occurrence.csv\")\n",
    "occurrences.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model we need some test exposure data. Lets have a look at an example Location and Account file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_locations = pd.read_csv('./gem/tests/data/dom-rep-146-oed-location.csv')\n",
    "test_locations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the model, we also need to define some analysis settings. Lets have a look at an example settings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gem/tests/analysis_settings.json', 'r') as myfile:\n",
    "    analysis_settings=json.loads(myfile.read().replace('\\n', ''))\n",
    "print(json.dumps(analysis_settings, indent=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the spatial index files used in the keys lookup\n",
    "! oasislmf model generate-peril-areas-rtree-file-index -c gem/keys_data/GMO/lookup.json -d gem/keys_data/GMO -f gem/keys_data/GMO/area-peril"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all the model files into Oasis binary formats\n",
    "! damagebintobin < gem/model_data/GMO/damage_bin_dict.csv > gem/model_data/GMO/damage_bin_dict.bin \n",
    "! evetobin < gem/model_data/GMO/events.csv > gem/model_data/GMO/events.bin\n",
    "! vulnerabilitytobin -d 166 < gem/model_data/GMO/vulnerability.csv > gem/model_data/GMO/vulnerability.bin\n",
    "! footprinttobin -i 313 < gem/model_data/GMO/footprint.csv\n",
    "! occurrencetobin -P 5000 -D < gem/model_data/GMO/occurrence.csv > gem/model_data/GMO/occurrence.bin\n",
    "! returnperiodtobin < gem/model_data/GMO/returnperiods.csv  > gem/model_data/GMO/returnperiods.bin\n",
    "! cp footprint.bin gem/model_data/GMO/\n",
    "! cp footprint.idx gem/model_data/GMO/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the model using the Oasis MDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf /tmp/analysis_test\n",
    "! oasislmf model run -C gem/oasislmf.json -r /tmp/analysis_test_mdk \\\n",
    "--source-exposure-file-path /tmp/exercise_1_oed/location.csv \\\n",
    "--source-accounts-file-path /tmp/exercise_1_oed/account.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_file\n",
    "from bokeh.models import ColumnDataSource, FactorRange\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.transform import factor_cmap\n",
    "\n",
    "analysis_directory = \"/tmp/analysis_test_mdk\"\n",
    "gul_aep = pd.read_csv(os.path.join(analysis_directory, \"output\", \"gul_S1_leccalc_full_uncertainty_aep.csv\"))\n",
    "gul_oep = pd.read_csv(os.path.join(analysis_directory, \"output\", \"gul_S1_leccalc_full_uncertainty_oep.csv\"))\n",
    "eps = pd.merge(gul_oep, gul_aep, on=[\"summary_id\", \"return_period\"], suffixes=[\"_oep\", \"_aep\"])\n",
    "eps = eps.sort_values(by=\"return_period\", ascending=True)\n",
    "return_periods = eps.return_period\n",
    "lec_types = ['OEP', 'AEP']\n",
    "data = {'Return periods' : return_periods,\n",
    "       'OEP': eps.loss_oep,\n",
    "       'AEP': eps.loss_aep}\n",
    "palette = [\"#c9d9d3\", \"#718dbf\"]\n",
    "x = [ (str(return_period), lec_type) for return_period in return_periods for lec_type in lec_types ]\n",
    "counts = sum(zip(data['OEP'], data['AEP']), ())\n",
    "source = ColumnDataSource(data=dict(x=x, counts=counts))\n",
    "p = figure(x_range=FactorRange(*x), plot_height=350, title=\"EP by return period\",\n",
    "          toolbar_location=None, tools=\"\")\n",
    "p.vbar(x='x', top='counts', width=0.9, source=source, line_color=\"white\",\n",
    "      fill_color=factor_cmap('x', palette=palette, factors=lec_types, start=1, end=2))\n",
    "p.y_range.start = 0\n",
    "p.x_range.range_padding = 0.1\n",
    "p.xaxis.major_label_orientation = 1\n",
    "p.xgrid.grid_line_color = None\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OasisWorkshop2018",
   "language": "python",
   "name": "oasisworkshop2018"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
